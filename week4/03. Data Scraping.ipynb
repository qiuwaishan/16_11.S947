{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Big Data And Society: Lab 3\n",
    "=====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Importing the library\n",
    "\n",
    "First we need to import the libraries, and some of their components. We will be using **Twython**, a library that provides wrappers around Twitter's API. To install **Twython** on a terminal or the command line, run the following command:\n",
    "```\n",
    "pip install twython\n",
    "```\n",
    "\n",
    "We also need to create a Python file that will contain the **Twitter** keys. It is never a good idea to host these keys on a public website like **github**, so one way to keep them private is importing the keys as a variable from a separate, untracked file. If you haven’t done so, at this point you should obtain your twitter API keys. Create a python file on the same directory of your **IPython** notebook, and name it: **`twitter_key.py`**.\n",
    "\n",
    "```\n",
    "# In the file you should define two variables:\n",
    "t_key = ‘your twitter key’\n",
    "t_secret = ‘your twitter secret’\n",
    "```\n",
    "\n",
    "Get your Twitter key and secret code on the [Twitter Apps site](https://apps.twitter.com/). You will need to create an application, it doesn't matter what you call it, to get your key and secret code. Once your application is created, you will find your keys on the **Keys and Access Tokens** tab of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## keys are here\n",
    "\n",
    "t_key = 'qUeiTMZV4PLkzRZ0ECqP5jE8b'\n",
    "t_secret = 'y0h1VYkj0aY5O6UcUZsk91Xevb2irF3qdaqvMZ2zgJLiu8imbW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from datetime import datetime\n",
    "from twython import Twython\n",
    "\n",
    "# Imports the keys from the python file\n",
    "from twitter_key import t_key, t_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Query the Twitter API\n",
    "Now we are going to construct a **Twython** object; this object simplifies the access to the [Twitter API](https://dev.twitter.com/overview/documentation), and provides methods for accessing the API’s endpoints. The first function fetches tweets with a given query at a given lat-long. We will be using the search parameters to hit the APIs endpoint. We need to provide the lat/lon of the centroid of the area we want to query, maximum number of tweets to return, and area within the centroid to search for, among others. \n",
    "\n",
    "Additional documentation of the Twython API can be found [here]( https://twython.readthedocs.org/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assigns the keys to the variables\n",
    "APP_KEY = t_key\n",
    "APP_SECRET = t_secret\n",
    "\n",
    "# Setup a Lat Lon\n",
    "latlong = [24.6333, 46.7167]\n",
    "\n",
    "''' Fetches tweets with a given query at a given lat-long.'''\n",
    "def get_tweets( latlong=None ):\n",
    "    # Creates a Twithon object with the given keys\n",
    "    twitter = Twython( APP_KEY, APP_SECRET )\n",
    "    # Uses the search function to hit the APIs endpoints and look for recent tweets within the area\n",
    "    results = twitter.search( geocode=','.join([ str(x) for x in latlong ]) + ',15km', result_type='recent', count=10000)\n",
    "    # Returns the only the statuses from the resulting JSON\n",
    "    return results['statuses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hit the API and Parse the Result\n",
    "We are going to create a function to help us repeatedly hit the API, and parse the result into a readable JSON that contains the things that we are interested in, and still stores the raw tweet as an additional property. The returned object is a Python `dict` that we can easily parse into another dictionary to later store as a JSON. Raw JSONs returned from the API have a specific structure. It can be sometimes hard to read a raw JSON. I find it easy to use some online parses like [this]( http://json.parser.online.fr/) to look at the structure of the JSON, and only access what we care about. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Does pretty much what its long name suggests. \"\"\"\n",
    "def get_lots_of_tweets( latlong ):\n",
    "    # Create a dictionary to parse the JSON\n",
    "    all_tweets = {}\n",
    "    \n",
    "    # We will be hitting the API a number of times within the total time\n",
    "    total_time = 120\n",
    "    # Everytime we hit the API we subtract time from the total\n",
    "    remaining_seconds = total_time\n",
    "    interval = 30 \n",
    "    while remaining_seconds > 0:\n",
    "        added = 0\n",
    "        # We hit the Twitter API\n",
    "        new_tweets = get_tweets( latlong )\n",
    "        # We parse the resulting JSON, and save the rest of the raw content\n",
    "        for tweet in new_tweets:\n",
    "            tid = tweet['id']\n",
    "            if tid not in all_tweets and tweet['coordinates'] != None:\n",
    "                properties = {}\n",
    "                properties['lat'] = tweet['coordinates']['coordinates'][0]\n",
    "                properties['lon'] = tweet['coordinates']['coordinates'][1]\n",
    "                properties['tweet_id'] = tid\n",
    "                properties['content'] = tweet['text']\n",
    "                properties['user'] = tweet['user']['id']\n",
    "                properties['user_location'] = tweet['user']['location']\n",
    "                properties['raw_source'] = tweet\n",
    "                properties['data_point'] = 'none'\n",
    "                properties['time'] = tweet['created_at']\n",
    "                all_tweets[ tid ] = properties\n",
    "                added += 1\n",
    "        print \"At %d seconds, added %d new tweets, for a total of %d\" % ( total_time - remaining_seconds, \n",
    "                                                                         added, len( all_tweets ) )\n",
    "        # We wait a few seconds and hit the API again\n",
    "        time.sleep(interval)\n",
    "        remaining_seconds -= interval\n",
    "    # We return the final dictionary\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the Functions\n",
    "\n",
    "We need to call the functions, and save the JSONs into a location. In this case, I made a folder called `twitter`, where I a m saving all the new JSONS. We can run the code continuously utilizing some loop, or we can use libraries like `threading`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0 seconds, added 3 new tweets, for a total of 3\n",
      "At 30 seconds, added 0 new tweets, for a total of 3\n",
      "At 60 seconds, added 2 new tweets, for a total of 5\n",
      "At 90 seconds, added 0 new tweets, for a total of 5\n"
     ]
    }
   ],
   "source": [
    "'''This function executes the rest of the functions over a given period of time'''\n",
    "def run():\n",
    "    # This is the number of times the code will be executed. In this case, just once. \n",
    "    starting = 1\n",
    "    while starting > 0:\n",
    "        # Sometimes the API returns some errors, killing the whole script, \n",
    "        # so we setup try/except to make sure it keeps running\n",
    "        try:\n",
    "            # We define a centroid in Riyadh\n",
    "            latlong = [24.6333, 46.7167]\n",
    "            t = get_lots_of_tweets( latlong )\n",
    "            # We name every file with the current time\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            # We write a new JSON into the target path\n",
    "            with open( '%stweets.json' %(timestr), 'w' ) as f:\n",
    "                f.write( json.dumps(t))\n",
    "            # we can use a library like threading to execute the run function continuously.\n",
    "            #threading.Timer(10, run).start()\n",
    "            starting -= 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parse the JSONs\n",
    "Once we have collected some data, we can parse it, and visualize some of the results. Since some of the data is repeated, we can initialize some lists to check whether or not a tweet already exists, and add it to the list. We can then extract the useful information for our purposes, and store it in another list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03. Data Scraping.ipynb', '20160302-154318tweets.json', '20160302-155509tweets.json', '20160302-170335tweets.json', '20160303-111333tweets.json', 'Bonus_DataScraping.ipynb']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c90feb9d4cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                     \u001b[0mlon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mall_pts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "# Import additional libraries\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the file names from a given directory\n",
    "file_dir = '/Users/WaishanQIU/Documents/big_data/16_11.S947/week4' # Set this to where your JSON saved\n",
    "onlyfiles = [ f for f in listdir(file_dir) if isfile(join(file_dir,f)) and not f.startswith('.')]\n",
    "print onlyfiles\n",
    "\n",
    "# Initialize some lists to store the points, and the ids of the tweets\n",
    "ids = []\n",
    "all_pts = []\n",
    "# Loop through all the files\n",
    "for file in onlyfiles:\n",
    "    full_dir = join(file_dir,file)\n",
    "    # Open the JSON\n",
    "    with open(full_dir) as f:\n",
    "        data = f.read()\n",
    "        # Load the JSON as a dict\n",
    "        dict = json.loads(data)\n",
    "        # Only add the unique tweets to the list\n",
    "        if not isinstance(dict, list):\n",
    "            for key, val in dict.iteritems():\n",
    "                if key not in ids:\n",
    "                    ids.append(key)\n",
    "                    lat = val['lat']\n",
    "                    lon = val['lon']\n",
    "                    all_pts.append([lat,lon])\n",
    "pts = np.array(all_pts)\n",
    "pts   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot some Tweets\n",
    "We can use **matplotlib** to visualize some tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-94d77d3b1811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use a scatter plot to make a quick visualization of the data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pts' is not defined"
     ]
    }
   ],
   "source": [
    "# Use a scatter plot to make a quick visualization of the data points\n",
    "plt.scatter(pts[:,0], pts[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that we don't get tweets plotted more than once. How would you make sure to only plot unique tweets? We can maybe:\n",
    "\n",
    "* Construct a list with unique id's\n",
    "* Only add the tweets to the `numpy.array` if the tweet doesn't exist in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem Set - Extend What you Learned\n",
    "\n",
    "Now that you know how to scrape data let's extend the exercise a little so you can show us what you know. This time you will set up the scraper to get data around MIT and scrap data for 15 minutes and visualize. Think about what you would need to change to do that. \n",
    "\n",
    "Once you have the new json file of Boston tweets you should make a new array so that you can make a new scatter plot of your Boston tweets. When you make this new array you should get at least two different attributes returned by the Twitter api. One of them should be the tweet id. Make sure you remove and duplicate tweets (if any). Plot the tweets with different colors (use lat/long to determine the colors) using the scatter plot tool. Then save the data to a CSV.\n",
    "\n",
    "Make sure you get your own Twitter Key.\n",
    "\n",
    "#### Deliverables\n",
    "\n",
    "**1** - Collect Tweets from Boston for 15 min. Note how you set the time in the above example, it was in seconds. How would you do that here? \n",
    "\n",
    "**2** - Plot your tweets using matplotlib.\n",
    "\n",
    "**3** - Change colors based on lat/long position.\n",
    "\n",
    "**4** - Save your array CSV file. We will be checking this CSV file for duplicates. So clean your file.  \n",
    "\n",
    "### What to Give Us on Stellar \n",
    "\n",
    "1 - Iphython notebook of your scrapper, which includes your scattterplot.\n",
    "\n",
    "2 - Your final CSV file. \n",
    "\n",
    "### Bonus # 1\n",
    "\n",
    "Bonus  -- Do the orginally planned assignment now titled Bonus. If you do the whole thing you get a free homework assignment. If  you do parts of it you will get points to future assignments. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
